{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66fc7f9a",
   "metadata": {},
   "source": [
    "# Explanatory section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d31d8",
   "metadata": {},
   "source": [
    "## Objective\n",
    "- this section is for explaining the implementation of the Walklets algorithm  \n",
    "and the other methods that were relevant for the testing phase  \n",
    "\n",
    "- this section also describes what problems were encountered and how were solved  \n",
    "in the development stage of this paper\n",
    "\n",
    "\n",
    "**Paper used for implementation:**  \n",
    "**\"Don't Walk, Skip! Online Learning of Multi-scale Network Embeddings\"**-->\n",
    "<https://arxiv.org/abs/1605.02115> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c1113f",
   "metadata": {},
   "source": [
    "## Algorithm explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67544c",
   "metadata": {},
   "source": [
    "- in this section I will describe the methods used from the `WalkletsEmbedder` class\n",
    "and also the methods that were used at the testing phase of the algorithm\n",
    "\n",
    "- first the project has the following essential files:\n",
    "- the *walklets_test.ipynb*, used for testing the algorithm implemented,  \n",
    "presents the implementation step-by-step in the **Implementation of algorithm** section\n",
    "- the *utils* is a module(package) used for functions called at the test phase\n",
    "- it contains the *utils.py* and since gensim was not upgraded to use the last numpy vers.  \n",
    "I used the *word2vec.py* reimplementation for the Walk2Vec Embedding Model\n",
    "- the files *random_walks.py* and *walklets.py* contains the implementation of both algorithms,  \n",
    "RandomWalk and Walklets which share the same platform(structure) for implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f04b5",
   "metadata": {},
   "source": [
    "### utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c763fe",
   "metadata": {},
   "source": [
    "- as mentioned above i will present some methods that I've added for visualising the results from  \n",
    "test phase\n",
    "\n",
    "- utils module contains utils.py, which was modified, and the word2vec.py and pyg_utils.py\n",
    "- this module was also used during the laboratories, and it is credited to\n",
    "https://github.com/zademn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9037247",
   "metadata": {},
   "source": [
    "#### `plot_similarity_heatmap()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a02c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_similarity_heatmap(\n",
    "    embeddings: np.ndarray,\n",
    "    node: int = 0,\n",
    "    cmap: str = \"hot_r\",\n",
    "    graph: nx.Graph = None,\n",
    "    labels: np.ndarray = None,\n",
    "    opt: int = 0,\n",
    "    figsize: tuple = (7, 7),\n",
    "    dpi: int = 100,\n",
    "    node_size: int = 20\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots t-SNE projection with class-based coloring (opt=0) or graph-colored by similarity to node (opt=1).\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): Embedding matrix.\n",
    "        node (int): Root node to compute similarity from (only used in opt=1).\n",
    "        cmap (str): Colormap.\n",
    "        graph (nx.Graph, optional): The graph to draw (required for opt=1).\n",
    "        labels (np.ndarray, optional): Node labels (required for opt=0).\n",
    "        opt (int): 0 = t-SNE with labels, 1 = graph colored by softmax similarity to root.\n",
    "        figsize (tuple): Figure size.\n",
    "        dpi (int): Plot resolution.\n",
    "        node_size (int): Node size for graph plot.\n",
    "    \"\"\"\n",
    "    if embeddings is None:\n",
    "        raise ValueError(\"Embeddings are not initialized. Please fit the model first.\")\n",
    "\n",
    "    if opt == 0:\n",
    "        if labels is None:\n",
    "            raise ValueError(\"Labels must be provided when opt=0 (t-SNE visualization).\")\n",
    "\n",
    "        tsne_emb = TSNE(n_components=2, random_state=42).fit_transform(embeddings)\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.scatter(tsne_emb[:, 0], tsne_emb[:, 1], c=labels, cmap=cmap)\n",
    "        plt.title(\"t-SNE projection of embeddings (colored by labels)\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        if graph is None:\n",
    "            raise ValueError(\"Graph must be provided when opt=1 (graph visualization).\")\n",
    "\n",
    "        raw_sims = embeddings @ embeddings[node].T\n",
    "        norm_sims = (raw_sims - raw_sims.min()) / (raw_sims.max() - raw_sims.min())*250\n",
    "        norm_sims[node]=0\n",
    "\n",
    "        pos = nx.spring_layout(graph, seed=42)\n",
    "        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "        nx.draw_networkx(\n",
    "            graph,\n",
    "            pos=pos,\n",
    "            with_labels=False,\n",
    "            node_color=norm_sims,\n",
    "            cmap=cmap,\n",
    "            node_size=node_size,\n",
    "            ax=ax\n",
    "        )\n",
    "        plt.title(f\"Graph coloring based on similarity rooted from node {node}\")\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap)\n",
    "        sm.set_array(norm_sims)\n",
    "        fig.colorbar(sm, ax=ax, label=\"Normalized Similarity*250\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d28ba",
   "metadata": {},
   "source": [
    "#####  Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436533f7",
   "metadata": {},
   "source": [
    "This function visualizes graph embeddings in two ways:\n",
    "\n",
    "**Option 0 — t-SNE Projection:**\n",
    "- Projects high-dimensional embeddings to 2D using **t-SNE**.\n",
    "- Colors each node based on its **class label**.\n",
    "- Ideal for visualizing how well the embedding separates different classes.\n",
    "- Used when performing community metric evaluation of larger networks\n",
    "\n",
    "**Option 1 — Graph Colored by Similarity:**\n",
    "- Computes **scaled dot-product similarity** between a selected `node`=root_node and all others using their embeddings.\n",
    "- Visualizes the graph using **NetworkX**, coloring nodes by their similarity.\n",
    "- Useful for analyzing how a node's neighborhood behaves in embedding space.\n",
    "- Used when performing community metric evaluation of smaller networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75694f36",
   "metadata": {},
   "source": [
    "#####  What was modified?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67017eda",
   "metadata": {},
   "source": [
    "- Initially there was a single option, that to plot only the graph by networkx draw, i wanted to use the  \n",
    "`draw_kamada_kawai` but this methodhad far more way parameters thus making it more tunable\n",
    "\n",
    "- When i choosed the datasets I've decided to use only a single network, a small one, but beacuse of a train time  \n",
    "pretty short, i've decided to intergate another dataset, medium to large one, that of Cora\n",
    "- When i've decided to test also on the large dataswet, i've created the option 0, that intially,  \n",
    "printed the heatmaps of every node with a color by its similarity(scale-dot product), but it wasnt feasable as i cannot see,  \n",
    "in an entire screen all the nodes heatmaps, or they were very small.\n",
    "- Then i've decied to move on plotting mechanism, and by this, isntaed of using the draw_networkx , that could be  \n",
    "very expensive in graphic generation, cause we have an enomrous quantity of nodes to be plotted, i've used plt from  \n",
    "`matplotlib` that plots the embedds, of course reduced from their intiial 128 dim to 2d dim by TSNE manifold learning algorithm\n",
    "- For option 1 or whatever is different than 0 initially i've used softmax applied to the dot products to see the simillarities,  \n",
    "as probabilities, but by testing i saw that it doesn't represent the dot-product very well, so i've switched to using the scale-dot  \n",
    "as normalized similarities, then multipling to 250 as the cmap has values ranging [0,250]\n",
    "- The cmap used is `hot_r` giving high to low similarities dark to lighter colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8c00b",
   "metadata": {},
   "source": [
    "#### `split_graph()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb3023",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def split_graph(\n",
    "    original_graph: nx.Graph,\n",
    "    labels: dict,\n",
    "    train_percentage: float,\n",
    "    seed: int = 42\n",
    "):\n",
    "    # 1. Set seed for reproducibility\n",
    "    random.seed(seed)\n",
    "\n",
    "    # 2. List of all nodes\n",
    "    all_nodes = list(original_graph.nodes())\n",
    "\n",
    "    # 3. Shuffle and split\n",
    "    random.shuffle(all_nodes)\n",
    "    train_size = int(train_percentage * len(all_nodes))\n",
    "    train_nodes = set(all_nodes[:train_size])\n",
    "    test_nodes = set(all_nodes[train_size:])\n",
    "\n",
    "    # 4. Build two subgraphs\n",
    "    graph_train = original_graph.subgraph(train_nodes).copy()\n",
    "    graph_test = original_graph.subgraph(test_nodes).copy()\n",
    "\n",
    "    # 5. Split labels\n",
    "    labels_train = {node: labels[node] for node in train_nodes if node in labels}\n",
    "    labels_test = {node: labels[node] for node in test_nodes if node in labels}\n",
    "\n",
    "    return graph_train, labels_train, graph_test, labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27524b2",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199693e",
   "metadata": {},
   "source": [
    "This function is used to **split a graph and its node labels into a training and testing subset**, preserving the graph structure within each split.\n",
    "\n",
    "To train the classification model on one part of the graph and evaluate it on another, we need to:\n",
    "- Randomly divide the nodes into training and test sets.\n",
    "- Create two subgraphs based on the original_graph: one for training and one for testing.\n",
    "- Assign the labels accordingly, to each node choosed randomly  \n",
    "\n",
    "The nodes returned will be used for searching their embeddings through `get_embedding_specific` or `get_scale_embedding_specific`  \n",
    "and feeding them to the classif model\n",
    "\n",
    "As we have the `train_test_split` method for dividing the features, the split_graph its build on the same concept, but  \n",
    "dividing nodes of the graph(network) instead of features of a dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f7c5b",
   "metadata": {},
   "source": [
    "### walklets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77724e9a",
   "metadata": {},
   "source": [
    "- this is the file in which the Walklets algorithm is implemented\n",
    "\n",
    "- in the first part of the file, there is short explanation of what are Walklets, what does the emb preserves compared  \n",
    "to original RandomWalk corpus 1-scaled(k=1)\n",
    "- the method shares the same platform as the *random_walks.py* file, as it has the same methods,  \n",
    "but it also exists cases where we use the methods from rw, like walk_graph\n",
    "- the methods suffered many changes along the development process\n",
    "- the algorithm its presented with its fields, and methods in the `WalkletsEmbedder` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35509396",
   "metadata": {},
   "source": [
    "#### `__init__()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d66df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def __init__(\n",
    "        self,\n",
    "        seed: int = 42,\n",
    "        walk_number: int = 1000,\n",
    "        walk_length: int = 11,\n",
    "        embed_dim: int = 128,\n",
    "        window_size: int = 3,\n",
    "        workers: int = 4,\n",
    "        epochs: int = 250,#enough for training time\n",
    "        learning_rate: float = 0.025,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The method initializes the Random Walk algorithm.\n",
    "        Returns:\n",
    "            nothing\n",
    "        Args:\n",
    "            seed (int): Random seed value. Default is 42.\n",
    "            walk_number (int): Number of random walks. Default is 1000.\n",
    "            walk_length (int): Length of random walks. Default is 11.\n",
    "            embed_dim (int): Dimensionality of embedding. Default is 128.\n",
    "            window_size (int): Size of the context window. Default is 3\n",
    "            e.g. walk->[0,1,2,3,4,5] and window_size=3 for node 4 word2vec will use as target: (4,1),(4,2),(4,3),(4,5)\n",
    "            workers (int): Number of cores to be used by the Word2Vec Model. Default is 4.\n",
    "            epochs (int): Number of epochs. Default is 500.\n",
    "            learning_rate (float): Learning rate of the Word2Vec Model. Default is 0.025.\n",
    "        \"\"\"\n",
    "        self.walk_number = walk_number\n",
    "        self.walk_length = walk_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.workers = workers\n",
    "        self.seed = seed\n",
    "        self.embeddings = None\n",
    "        self.node_to_idx = None\n",
    "        np.random.seed(self.seed)  # Set the random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb0f69",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c1d9f",
   "metadata": {},
   "source": [
    "- as simple as it is, this file its the constructor of the `WalkletsEmbedder` class, and it instances  \n",
    "the params of the model with the actual parameters that has got or if they are not given, the constructor  \n",
    "uses a standard parameters\n",
    "\n",
    "- along this parameters we have  \n",
    "walk_number: int = 1000, value used in the ResearchPaper  \n",
    "walk_length: int = 11, value used in the ResearchPaper  \n",
    "embed_dim: int = 128, value used in the ResearchPaper  \n",
    "epochs: int = 250, enough for training time, due to hardware limitations  \n",
    "learning_rate: float = 0.025, value used in the ResearchPaper\n",
    "- the workers arg offers a multitasking capability that wasn't implemented anymore\n",
    "- we also use for this class a dict where we save the embeddings for each scale-->`self.scale_emebeddings`\n",
    "- this feild will be useful when storing the trained emb. and also when we will want to search for a emb of a node\n",
    "- beacause we store the emb for each scales idnivdually we are not concatenate them, so we will not use the `self.embeddings` field\n",
    "- we also use in this class the scales, an array which will store the int 's of the scales that were used and present in the  \n",
    "`self.scale_embeddings`\n",
    "- this will be useful when a user wants to get the embeddings for a scale or the embedding for a node contained in a scale, so by this  \n",
    "we verify if the user wants a scale that was fitted first\n",
    "- and we also use a dict type `self.node_to_idx` to store {str of the node:index of embed} so we can access later on  \n",
    "the embeddings by having a list of nodes List[Any] given as parameters for another methods that will find their emb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe2969",
   "metadata": {},
   "source": [
    "##### What was modified?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192bd39d",
   "metadata": {},
   "source": [
    "- at the beggining versions the algorithm, could make embeddings just for a single scale, and store them all in the  \n",
    "`self.embeddings`\n",
    "\n",
    "- but when decided to train also on a larger networks, this structure proven to be very high time and resource consumer,  \n",
    "because at each scale we would train the algorithm, would need to remake the randomwalk(the initial corpus)\n",
    "- so i compressed all scales in a single fit call that gets a list of scales, and the randomwalk are made once, and then on it  \n",
    "is made the corpus for the walklets and init of word2vec and training process, and the traiend embedds are stored in an organized  strcuture `self.scale_embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b67e53",
   "metadata": {},
   "source": [
    "#### `select_walklets()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f14a58",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " @staticmethod\n",
    "    def _select_walklets(walks: List[List[str]], scale: int=2) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Extracts nodes that are separated by 'scale' steps in each walk to form a new corpus.\n",
    "        Args:\n",
    "            walks (List[List[str]]): The list of walks where each walk is a list of node IDs as strings\n",
    "            scale (int): The scale k at which to extract nodes   \n",
    "        Returns:\n",
    "            List[List[str]]: A list of walklet sequences with nodes that are scale steps apart\n",
    "        \"\"\"\n",
    "        walklets = []\n",
    "        for walk in walks:\n",
    "            if len(walk) <= scale:\n",
    "                continue\n",
    "            \n",
    "            # Create a new walk with nodes that are scale steps apart\n",
    "            walklet = [walk[i] for i in range(0, len(walk), scale)]\n",
    "            if len(walklet) > 1:  # Only add walks with at least 2 nodes\n",
    "                walklets.append(walklet)\n",
    "        \n",
    "        return walklets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d663e",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33ba3b",
   "metadata": {},
   "source": [
    "This static method takes a list of random walks and selects only those nodes that are separated by a **fixed number of steps (`scale`)** within each walk.\n",
    "\n",
    "It's a core part of the **Walklets algorithm**, which captures multi-scale context by:\n",
    "- Skipping over intermediate nodes.\n",
    "- Producing a new \"corpus\" of node sequences where only distant relationships are preserved.\n",
    "\n",
    "This means that a node will capture information about nodes that are at a farther distance, e.g another comunity\n",
    "- Walklets use this method to create different views of the graph at **multiple scales** (e.g., local vs. global structure).\n",
    "- These walklets are then fed into Word2Vec to learn embeddings that capture relationships at scale `k`.\n",
    "\n",
    "- First it checks if the scale needed its far way larger than the `walk_length`, implcitly the length of the current walk, cause its the max limit a scale could be\n",
    "- Then if lower it will iterate with a for on the current walk, with step=scale, obtaining a new list with indexes extracted at scale steps \n",
    "\n",
    "*Observe that the method its a static one, we could call it once we have the walks generated by walk_graph() from RandomWalkEmbedder*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ed47dd",
   "metadata": {},
   "source": [
    "#### `fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d182960",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    def fit(self, original_graph: nx.classes.graph.Graph = nx.karate_club_graph(), scales: Union[List[int], int] = 1):\n",
    "        \"\"\"\n",
    "        The method fits the Walklets model to the graph.\n",
    "        It will create random walks on the graph provided, then for each scale it will\n",
    "        extract nodes that are k distance apart from these walks, creating separate corpus for each scale.\n",
    "        Then it will train a Word2Vec model for each scale and concatenate all embeddings.\n",
    "        \n",
    "        Returns:\n",
    "            nothing\n",
    "        Args:\n",
    "            original_graph (nx.Graph): The original graph to be used for embedding. Default is the karate club graph.\n",
    "            scales (Union[List[int], int]): The scale(s) of the walklets. Can be a single integer or a list of integers.\n",
    "                                           Default is 1 (equivalent to RandomWalk).\n",
    "        \"\"\"\n",
    "        # Convert single scale to list for consistent processing\n",
    "        if isinstance(scales, int):\n",
    "            scales = [scales]\n",
    "        \n",
    "        # Save scales used\n",
    "        self.scales = scales\n",
    "        \n",
    "        # Initialize node to index mapping\n",
    "        self.node_to_idx = {str(node): idx for idx, node in enumerate(original_graph.nodes())}\n",
    "        num_of_nodes = original_graph.number_of_nodes()\n",
    "        \n",
    "        # Generate random walks using the static method from RandomWalkEmbedder\n",
    "        # We generate the walks once and reuse them for different scales\n",
    "        walks = RandomWalkEmbedder.walk_graph(original_graph, self.walk_number, self.walk_length)\n",
    "        print(f\"Generated a total of {len(walks)} random walks of length {self.walk_length}, starting from {len(walks)/self.walk_number} nodes.\\n\")\n",
    "        \n",
    "        # Initialize embeddings array\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # Process each scale\n",
    "        for scale in scales:\n",
    "            print(f\"Processing scale k={scale}\")\n",
    "            \n",
    "            # Extract node sequences that are 'scale' steps apart\n",
    "            scale_corpus = self._select_walklets(walks, scale)\n",
    "            print(f\"Extracted {len(scale_corpus)} walklets for scale k={scale}\")\n",
    "            \n",
    "            # Train custom Word2Vec model\n",
    "            model = Word2Vec(\n",
    "                sequences=scale_corpus,\n",
    "                embedding_dim=self.embed_dim,\n",
    "                window_size=self.window_size,  # Using standard window size for the new corpus\n",
    "                negative_samples=5  # Default value for negative samples\n",
    "            )\n",
    "            \n",
    "            # Train the model with specified epochs and batch size\n",
    "            model.train(epochs=self.epochs, batch_size=1024)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            # Create the embeddings array for this scale\n",
    "            scale_embeddings = np.zeros((num_of_nodes, self.embed_dim))\n",
    "            for node in original_graph.nodes():\n",
    "                node_str = str(node)\n",
    "                vector = model.get_word_vector(node_str)\n",
    "                if vector is not None:\n",
    "                    scale_embeddings[self.node_to_idx[node_str]] = vector\n",
    "            \n",
    "            # Store this scale's embeddings\n",
    "            self.scale_embeddings[scale] = scale_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce45919f",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d624b94",
   "metadata": {},
   "source": [
    "This method trains the **Walklets** model on a given graph. It performs the following steps:\n",
    "\n",
    "1. **Prepare scales**: Convert a single scale value to a list for unified processing.\n",
    "\n",
    "2. **Generate random walks**: Reuse these walks across all scales.\n",
    "3. **For each scale `k`**:\n",
    "   - Select walklets by skipping `k-1` nodes in walks.(make the corpus via `select_walks`)\n",
    "   - Train a Word2Vec model on these walklets.\n",
    "   - Extract node embeddings and store them by scale.\n",
    "\n",
    "- This method uses batch_size=1024 for organizing the dataset used for Word2Vec\n",
    "\n",
    "- After the Word2Vec trains the embedds, we take all the nodes from the graph given as the parameter, and  \n",
    "get through `get_word_vector` the emb of that node, than by using the `self.node_to_idx` we store the emb, in the  \n",
    "`self.scale_embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda15537",
   "metadata": {},
   "source": [
    "##### What was modified?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0abb789",
   "metadata": {},
   "source": [
    "- As stated above, the agorithm here was modified, intially from computing the rwalks then computing the walklets for a single scale, then training the Word2Vec, to computing the rwalks once and compute the walklets and train the emb for multiple scales, scales given  through `scales` paramater as a List of int' s or an int (stored as union structure type, mutual exclusive structure)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bebe72f",
   "metadata": {},
   "source": [
    "#### `get_scale_embedding()` && `get_scale_embedding_specific()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb795e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_scale_embedding(self, scale: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The method returns embeddings for a specific scale.\n",
    "        \n",
    "        Args:\n",
    "            scale (int): The scale for which to get embeddings\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: A 2D array of node embeddings for the specified scale\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If the model has not been fitted or if the scale is not available\n",
    "        \"\"\"\n",
    "        if not self.scale_embeddings:\n",
    "            raise ValueError(\"Model has not been fitted. Call fit() first.\")\n",
    "        \n",
    "        if scale not in self.scales:\n",
    "            raise ValueError(f\"Scale {scale} not found. Available scales: {self.scales}\")\n",
    "        \n",
    "        return self.scale_embeddings[scale]\n",
    "    \n",
    "    def get_scale_embedding_specific(self, nodes: List[Any], scale: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The method gets embeddings only for the specified list of nodes at a specific scale.\n",
    "    \n",
    "        Args:\n",
    "            nodes (List[Any]): List of node IDs (the IDs as they are in the graph)\n",
    "            scale (int): The scale to get embeddings for\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Embeddings for the specified nodes at the specified scale.\n",
    "                        Shape (len(nodes), embed_dim)\n",
    "                   \n",
    "        Raises:\n",
    "            ValueError: If the model has not been fitted or if the scale is not available\n",
    "        \"\"\"\n",
    "        if not self.scale_embeddings or not self.scales:\n",
    "            raise ValueError(\"Model has not been fitted. Call fit() first.\")\n",
    "    \n",
    "        if scale not in self.scales:\n",
    "            raise ValueError(f\"Scale {scale} not found. Available scales: {self.scales}\")\n",
    "    \n",
    "        # Get the embeddings for the specified scale\n",
    "        scale_embeddings = self.scale_embeddings[scale]\n",
    "    \n",
    "        # Get the indices for the specified nodes\n",
    "        idxs = [self.node_to_idx[str(node)] for node in nodes]\n",
    "    \n",
    "        # Return the embeddings for the specified nodes at the specified scale\n",
    "        return scale_embeddings[idxs]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e2b2a",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386464c9",
   "metadata": {},
   "source": [
    "After training the Walklets model, we often want to:\n",
    "\n",
    "- Extract the **entire embedding matrix** for a given scale `k`.\n",
    "\n",
    "- Retrieve **specific node embeddings** for a subset of nodes at scale `k`.\n",
    "\n",
    "Such operations are descirbed byt the following methods:\n",
    "\n",
    "**Method 1**: `get_scale_embedding(scale)`\n",
    "- Returns the full embedding matrix for a specified scale(all the node's embeddings for a scale)\n",
    "- Shape returned: `(len(self.scale_embeddings[...]), embedding_dim)`\n",
    "\n",
    "**Method 2**: `get_scale_embedding_specific(nodes, scale)`\n",
    "- Returns embeddings only for a given list of nodes at a specified scale.\n",
    "- Shape: `(len(nodes), embedding_dim)`\n",
    "\n",
    "- This methods are based on the `get_embedding` and `get_embedding_specific` from `RandomWalkEmbedder` class  \n",
    "but working on different scales rather than a sngle scale k=1, we will build a method that gets the embeddings,  \n",
    "for all the nodes trained on a scale=k, and for the other one we will build a method that gets the emb of the nodes  \n",
    "gaved by param from a given scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab285ea",
   "metadata": {},
   "source": [
    "#### `store_emb()` && `load_emb()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d0658",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " def store_emb(self, path: str):\n",
    "        \"\"\"\n",
    "        Stores the embeddings and node mapping to files.\n",
    "        Args:\n",
    "            path (str): Path where to save the embeddings\n",
    "        \"\"\"\n",
    "        if not self.scale_embeddings or not self.scales:\n",
    "            raise ValueError(\"Model has not been fitted. Call fit() first.\")\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        # Save individual scale embeddings\n",
    "        for scale in self.scales:\n",
    "            scale_path = os.path.join(os.path.dirname(path), os.path.basename(path) + f\"_scale_{scale}_embeddings.npy\")\n",
    "            np.save(scale_path, self.scale_embeddings[scale])\n",
    "        \n",
    "        # Save scales list\n",
    "        scales_path = os.path.join(os.path.dirname(path), os.path.basename(path) + \"_scales.json\")\n",
    "        with open(scales_path, 'w') as f:\n",
    "            json.dump(self.scales, f)\n",
    "        \n",
    "        # Save node mapping\n",
    "        mapping_path = os.path.join(os.path.dirname(path), os.path.basename(path) + \"_mapping.json\")\n",
    "        with open(mapping_path, 'w') as f:\n",
    "            json.dump({str(k): int(v) for k, v in self.node_to_idx.items()}, f)\n",
    "    \n",
    "    def load_emb(self, path: str) -> Tuple[Dict[str, int], List[int], Dict[int, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Loads embeddings, node mapping, and scales from files.\n",
    "        Args:\n",
    "            path (str): Path to the saved embeddings (without file extensions)\n",
    "        Returns:\n",
    "            Tuple[Dict[str, int], List[int], Dict[int, np.ndarray]]: \n",
    "                - Node to index mapping\n",
    "                - List of scales\n",
    "                - Dictionary of scale-specific embeddings\n",
    "        \"\"\"\n",
    "        # Construct paths\n",
    "        mapping_path = os.path.join(os.path.dirname(path), os.path.basename(path) + \"_mapping.json\")\n",
    "        scales_path = os.path.join(os.path.dirname(path), os.path.basename(path) + \"_scales.json\")\n",
    "\n",
    "        if not os.path.exists(mapping_path) or not os.path.exists(scales_path):\n",
    "            raise FileNotFoundError(\"Mapping or scales file not found.\")\n",
    "    \n",
    "        # Load node mapping\n",
    "        with open(mapping_path, 'r') as f:\n",
    "            self.node_to_idx = json.load(f)\n",
    "            self.node_to_idx = {k: int(v) for k, v in self.node_to_idx.items()}\n",
    "    \n",
    "        # Load scales\n",
    "        with open(scales_path, 'r') as f:\n",
    "            self.scales = json.load(f)\n",
    "    \n",
    "        # Load scale-specific embeddings\n",
    "        for scale in self.scales:\n",
    "            scale_path = os.path.join(os.path.dirname(path), os.path.basename(path) + f\"_scale_{scale}_embeddings.npy\")\n",
    "            if os.path.exists(scale_path):\n",
    "                self.scale_embeddings[scale] = np.load(scale_path)\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"Embeddings for scale {scale} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae089675",
   "metadata": {},
   "source": [
    "##### Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029e903",
   "metadata": {},
   "source": [
    "This methods are useful for large networks, for storing their trained params.\n",
    "\n",
    "- Because we will have large datasets(Cora) to train on, we will have a bigger execution time\n",
    "- Instead of training once more, we will store the emb once the `fit` was called, then whenever  \n",
    "when you want to test the emb, you will, instead of calling `fit`, call `load` and use the test cases in the  \n",
    "`walklets_test.ipynb` as normal\n",
    "- Observe that store, will store not only the embeddings but the other relevant parameter too\n",
    "- The model will save its `self.node_to_idx`, so when we will load, we could use the methods\n",
    "`get_scale_embedding_specific` and `get_scale_embedding` without problems\n",
    "- In this way the model will save its `self.scale_embeddings` and `self.scales` to be used after we call the `load`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db688303",
   "metadata": {},
   "source": [
    "##### What was modified?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143e56e1",
   "metadata": {},
   "source": [
    "- Intially when i've tested on a small network(KarateClub) only, it wouldn't be ncecesarly to store the model's parameters\n",
    "\n",
    "- But as we use larger and larger networks , the time needed for executing the training phase will grow expon.\n",
    "- As we will want to test the emb through different tests later on, **this step is mandatory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f791425b",
   "metadata": {},
   "source": [
    "### random_walks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb32e3d",
   "metadata": {},
   "source": [
    "-  this file implements the RandomWalk algorithm using the `RandomWalkEmbedder` class\n",
    "\n",
    "- the RandomWalk algorithm uses in the same fashion as Walklets the corpus obtained via RandomWalk algorithm,  \n",
    "but this time leaving the walks as they are, at k=scale=1\n",
    "\n",
    "- the methods are likewise `WalkletsEmbedder` class, but instead of using different scales we will use a single scale, k=1\n",
    "- this means that all the methods that used mutliple scales will change to use a single scale\n",
    "- by this we reffer to the `fit` method, which will only make the rwalks through `walk_graph` method  \n",
    "and train directly on that corpus\n",
    "- by this we reffer to the `get_embedding` and `get_embedding_specific` methods, which will return all the embeddings  \n",
    "here stored in the field `self.embeddings`, respectively return the emb for certain nodes\n",
    "- in the store and load we store and load only the `self.embeddings` and `self.node_to_idx`\n",
    "- all the other methods ar the same as `WalkletsEmbedder` class\n",
    "- the `walk_graph` and `walk_node` will be static methods , which, will  \n",
    "\n",
    "1. `walk_graph`-->for all the nodes in the graph will call `walk_node` for `walk_number` times ,making `walk_number` walks, that  \n",
    "are preserved in a List[]\n",
    "2. `walk_number`-->makes a walk by randomly choosing on which neigbour of the current node to move foward, than making this for `walk_length` times, thus creating a walk that will be stored in the List of walks  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
